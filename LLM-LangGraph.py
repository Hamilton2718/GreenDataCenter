from typing import TypedDict, Annotated, List, Dict, Optional
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
# é€šè¿‡é€šç”¨é…ç½®æ¥æ¥å…¥å¤§æ¨¡å‹
from langchain_openai import ChatOpenAI
import operator
import json
import os
from IPython.display import Image, display
from PIL import Image
# 1. å®šä¹‰çŠ¶æ€ç»“æ„
class DataCenterState(TypedDict):
    # è¾“å…¥å‚æ•°
    green_energy_ratio: float  # ç»¿ç”µå æ¯” (0-1)
    computing_surplus: float  # ç®—åŠ›å¯Œä½™åº¦ (0-1)
    network_latency: float  # ç½‘ç»œå»¶è¿Ÿ (ms)
    carbon_intensity: float  # ç¢³æ’å¼ºåº¦ (gCO2/kWh)
    latency_requirement: float  # å»¶è¿Ÿè¦æ±‚ (ms)

    # é€»è¾‘è®¡ç®—å¾—åˆ°çš„ä¸­é—´ç»“æœ
    analysis_result: Dict
    migration_path: List[str]
    energy_storage_strategy: Dict
    green_energy_allocation: Dict

    # å¤§æ¨¡å‹ç”Ÿæˆçš„è§è§£ä¸ä¼˜åŒ–å»ºè®®
    llm_insights: str

    # äººå·¥å®¡æ ¸çŠ¶æ€
    human_feedback: Optional[str]
    approved: Optional[bool]

    # æœ€ç»ˆè¾“å‡º
    final_plan: Dict
    messages: Annotated[List[BaseMessage], operator.add]


# 2. åˆå§‹åŒ–å¤§æ¨¡å‹ (ä½¿ç”¨ XSimple æä¾›çš„æ¨¡å‹æ¥å£æˆ–å…¼å®¹æ¥å£)
# è¯·æ›¿æ¢ä¸ºå®é™…å¯ç”¨çš„ API Key å’Œ Base URL
llm = ChatOpenAI(
    model="qwen-plus",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)


# --- èŠ‚ç‚¹å®šä¹‰ ---

# èŠ‚ç‚¹ 1: åŸºç¡€æ•°æ®è®¡ç®— (ç¡¬é€»è¾‘)
def analyze_datacenter(state: DataCenterState) -> DataCenterState:
    green_ratio = state["green_energy_ratio"]
    carbon = state["carbon_intensity"]

    # æ‰‹åŠ¨é€»è¾‘ï¼šç®€å•çš„è¯„åˆ†
    green_score = green_ratio * 100
    carbon_score = max(0, 100 - carbon / 10)

    state["analysis_result"] = {
        "green_score": green_score,
        "carbon_score": carbon_score,
        "status": "Green" if green_score > 60 else "Carbon-Heavy"
    }
    state["messages"].append(AIMessage(content="âœ… å·²å®ŒæˆåŸºç¡€æ•°æ®åˆæ­¥åˆ†æ"))
    return state


# èŠ‚ç‚¹ 2: è¿ç§»å†³ç­– (ç»“åˆç¡¬é€»è¾‘)
def plan_migration_path(state: DataCenterState) -> DataCenterState:
    # æ¨¡æ‹Ÿè·¯å¾„è§„åˆ’
    if state["network_latency"] <= state["latency_requirement"]:
        path = ["Region_A_Green_DC", "Region_B_Edge"]
    else:
        path = ["Internal_Optimized_Node"]

    state["migration_path"] = path
    state["messages"].append(AIMessage(content="âœ… è¿ç§»è·¯å¾„è§„åˆ’å®Œæˆ"))
    return state


# èŠ‚ç‚¹ 3: ç»¿ç”µåˆ†é… (ç¡¬é€»è¾‘)
def allocate_green_energy(state: DataCenterState) -> DataCenterState:
    green_ratio = state["green_energy_ratio"]
    allocation = {
        "AI_training": f"{min(green_ratio * 0.6, 0.5) * 100:.1f}%",
        "critical_services": f"{min(green_ratio * 0.3, 0.3) * 100:.1f}%",
        "total_usage": f"{green_ratio * 100:.1f}%"
    }
    state["green_energy_allocation"] = allocation
    state["messages"].append(AIMessage(content="âœ… ç»¿ç”µé…é¢åˆ†é…å®Œæˆ"))
    return state


# èŠ‚ç‚¹ 4: LLM æ™ºèƒ½åˆ†æèŠ‚ç‚¹ (å…³é”®è¡¥å……ï¼šè°ƒç”¨ API)
def llm_reasoning_node(state: DataCenterState) -> DataCenterState:
    """åˆ©ç”¨å¤§æ¨¡å‹å¯¹ä¸Šè¿°æ‰€æœ‰æŠ€æœ¯å‚æ•°è¿›è¡Œç»¼åˆè¯„ä¼°å¹¶è¾“å‡ºæ·±åº¦å»ºè®®"""

    base_prompt = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç»¿è‰²æ•°æ®ä¸­å¿ƒè°ƒåº¦ä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹è¿è¡Œæ•°æ®ï¼Œè¯·æä¾›ä¸€æ®µä¸“ä¸šçš„è°ƒåº¦å»ºè®®ï¼ˆçº¦150å­—ï¼‰ï¼š
    - ç»¿ç”µå æ¯”: {green_ratio}% 
    - ç¢³æ’å¼ºåº¦: {carbon} gCO2/kWh
    - ç½‘ç»œå»¶è¿Ÿ: {latency}ms (é˜ˆå€¼: {latency_req}ms)
    - é¢„å®šè¿ç§»è·¯å¾„: {path}
    - ç»¿ç”µåˆ†é…æ–¹æ¡ˆ: {allocation}

    è¯·é‡ç‚¹å›ç­”ï¼šå½“å‰æ–¹æ¡ˆåœ¨é™ä½ç¢³æ’å’Œä¿éšœå»¶è¿Ÿä¹‹é—´æ˜¯å¦è¾¾åˆ°äº†å¹³è¡¡ï¼Ÿ
    """

    # å¦‚æœæœ‰äººå·¥åé¦ˆï¼Œæ·»åŠ åˆ° prompt ä¸­
    if state.get("human_feedback"):
        base_prompt += f"\n\næ³¨æ„ï¼šç”¨æˆ·å¯¹ä¸Šä¸€ç‰ˆæ–¹æ¡ˆçš„åé¦ˆå¦‚ä¸‹ï¼Œè¯·åŠ¡å¿…æ ¹æ®æ­¤åé¦ˆè¿›è¡Œé’ˆå¯¹æ€§è°ƒæ•´å’Œä¼˜åŒ–ï¼š\n{state['human_feedback']}"

    prompt = ChatPromptTemplate.from_template(base_prompt)

    chain = prompt | llm
    response = chain.invoke({
        "green_ratio": state["green_energy_ratio"] * 100,
        "carbon": state["carbon_intensity"],
        "latency": state["network_latency"],
        "latency_req": state["latency_requirement"],
        "path": state["migration_path"],
        "allocation": json.dumps(state["green_energy_allocation"])
    })

    state["llm_insights"] = response.content
    state["messages"].append(AIMessage(content="ğŸ¤– LLM ä¸“å®¶æ™ºèƒ½å»ºè®®å·²ç”Ÿæˆ"))
    return state


# èŠ‚ç‚¹ 5: ç”Ÿæˆæœ€ç»ˆæ–¹æ¡ˆæ ¼å¼åŒ–
def generate_final_plan(state: DataCenterState) -> DataCenterState:
    final_plan = {
        "metrics": {
            "green_ratio": f"{state['green_energy_ratio'] * 100}%",
            "latency_compliant": state["network_latency"] <= state["latency_requirement"]
        },
        "path": state["migration_path"],
        "allocation": state["green_energy_allocation"],
        "expert_advice": state["llm_insights"]
    }
    state["final_plan"] = final_plan
    state["messages"].append(AIMessage(content="âœ… å®Œæ•´è°ƒåº¦è®¡åˆ’å¯¼å‡ºæˆåŠŸ"))
    return state


# èŠ‚ç‚¹ 6: äººå·¥å®¡æ ¸èŠ‚ç‚¹
def human_review_node(state: DataCenterState) -> DataCenterState:
    print("\n" + "="*30)
    print("=== äººå·¥å®¡æ ¸ç¯èŠ‚ ===")
    print("="*30)
    print(f"å½“å‰ LLM å»ºè®®:\n{state['llm_insights']}")
    print("-" * 30)
    
    while True:
        user_input = input("\nè¯·è¾“å…¥å®¡æ ¸æ„è§ (è¾“å…¥ 'pass' æˆ– 'ok' é€šè¿‡ï¼Œå¦åˆ™è¾“å…¥å…·ä½“ä¿®æ”¹å»ºè®®): ").strip()
        if user_input:
            break
    
    if user_input.lower() in ['pass', 'ok', 'yes', 'é€šè¿‡']:
        print(">>> å®¡æ ¸é€šè¿‡ï¼Œæµç¨‹ç»“æŸã€‚")
        state["messages"].append(HumanMessage(content="å®¡æ ¸é€šè¿‡"))
        return {"human_feedback": None, "approved": True}
    else:
        print(f">>> å®¡æ ¸ä¸é€šè¿‡ï¼Œåé¦ˆæ„è§å·²è®°å½•: {user_input}")
        print(">>> æ­£åœ¨é‡æ–°ç”Ÿæˆæ–¹æ¡ˆ...")
        state["messages"].append(HumanMessage(content=f"å®¡æ ¸ä¸é€šè¿‡ï¼Œæ„è§: {user_input}"))
        return {"human_feedback": user_input, "approved": False}


def review_router(state: DataCenterState):
    """æ ¹æ®äººå·¥å®¡æ ¸ç»“æœå†³å®šä¸‹ä¸€æ­¥"""
    if state.get("approved"):
        return END
    else:
        return "llm_reasoning"



# --- æ„å»º LangGraph å·¥ä½œæµ ---

def create_scheduling_graph():
    workflow = StateGraph(DataCenterState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("analyze", analyze_datacenter)
    workflow.add_node("plan_migration", plan_migration_path)
    workflow.add_node("allocate_green", allocate_green_energy)
    workflow.add_node("llm_reasoning", llm_reasoning_node)  # æ–°å¢ LLM èŠ‚ç‚¹
    workflow.add_node("generate_plan", generate_final_plan)
    workflow.add_node("human_review", human_review_node)    # æ–°å¢äººå·¥å®¡æ ¸èŠ‚ç‚¹

    # å®šä¹‰è¾¹ï¼ˆæ‰§è¡Œé¡ºåºï¼‰
    workflow.set_entry_point("analyze")
    workflow.add_edge("analyze", "plan_migration")
    workflow.add_edge("plan_migration", "allocate_green")
    workflow.add_edge("allocate_green", "llm_reasoning")  # è¿æ¥åˆ° LLM
    workflow.add_edge("llm_reasoning", "generate_plan")  # ä» LLM è¿æ¥åˆ°ç”Ÿæˆ
    
    # generate_plan åè¿æ¥åˆ°äººå·¥å®¡æ ¸ï¼Œè€Œä¸æ˜¯ç›´æ¥ç»“æŸ
    workflow.add_edge("generate_plan", "human_review")
    
    # æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®å®¡æ ¸ç»“æœå†³å®šæ˜¯ç»“æŸè¿˜æ˜¯é‡åš
    workflow.add_conditional_edges(
        "human_review",
        review_router,
        {
            "llm_reasoning": "llm_reasoning", # å¦‚æœä¸é€šè¿‡ï¼Œå›é€€åˆ° LLM æ¨ç†èŠ‚ç‚¹
            END: END                          # å¦‚æœé€šè¿‡ï¼Œç»“æŸ
        }
    )

    return workflow.compile()
#æ­¤å¤„åªç»™å‡ºäº†å•å‘æ— åˆ†æ”¯å›¾çš„æ„å»ºæ–¹æ³•ï¼Œå®é™…ä¸­å¯ä»¥ç»§ç»­æ„é€ åˆ†æ”¯ç»“æ„å’Œå¾ªç¯ç»“æ„

# --- æ‰§è¡Œå…¥å£ ---

def main():
    app = create_scheduling_graph()

    # å°è¯•å¯è§†åŒ–å¤„ç†

    try:
            graph_image_path = "datacenter_workflow.png"
            # ä½¿ç”¨ Mermaid ç”Ÿæˆå›¾ç‰‡ (è¿”å›äºŒè¿›åˆ¶æ•°æ®)
            graph_png = app.get_graph().draw_mermaid_png()
            with open(graph_image_path, "wb") as f:
                f.write(graph_png)
            print(f"\n--- LangGraph æµç¨‹å›¾å·²ä¿å­˜è‡³: {graph_image_path} ---")

            # ä½¿ç”¨ Pillow æ‰“å¼€å¹¶æ˜¾ç¤ºå›¾ç‰‡
            img = Image.open(graph_image_path)
            img.show()  # è¿™ä¼šæ‰“å¼€ä¸€ä¸ªæ–°çš„çª—å£æ˜¾ç¤ºå›¾ç‰‡
            print("\n--- æµç¨‹å›¾å·²åœ¨æ–°çš„å›¾ç‰‡æŸ¥çœ‹å™¨çª—å£ä¸­æ˜¾ç¤º ---")

    except Exception as e:
            print(f"\n--- æ— æ³•ç”Ÿæˆæµç¨‹å›¾ã€‚è¯·ç¡®ä¿ç½‘ç»œç•…é€šæˆ–å·²é…ç½®ç›¸å…³ç¯å¢ƒã€‚é”™è¯¯: {e} ---")

        # å¯è§†åŒ–å¤„ç†ç»“æŸ

    # è¾“å…¥ä¸€ç»„æœ‰å¾…è¯„ä¼°çš„æ•°æ®ä¸­å¿ƒå‚æ•°
    initial_state = {
        "green_energy_ratio": 0.65,
        "computing_surplus": 0.25,
        "network_latency": 18.0,
        "carbon_intensity": 450.0,
        "latency_requirement": 30.0,
        "messages": [HumanMessage(content="å¯åŠ¨æ•°æ®ä¸­å¿ƒè°ƒåº¦åˆ†ææµç¨‹")]
    }


    print("--- æ­£åœ¨å¯åŠ¨ XSimple ç»¿è‰²ç®—åŠ›è°ƒåº¦æµç¨‹ ---")
    result = app.invoke(initial_state)

    print("\n[æœ€ç»ˆæ–¹æ¡ˆæ¦‚è§ˆ]")
    print(f"å»ºè®®å†…å®¹: {result['final_plan']['expert_advice']}")

    # print("\n[æ‰§è¡Œæ—¥å¿—]")
    # for msg in result["messages"]:
    #     if isinstance(msg, AIMessage):
    #         print(f" - {msg.content}")


if __name__ == "__main__":
    main()